{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d072d421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyteomics\n",
    "import pandas as pd\n",
    "import time\n",
    "from pyteomics import mzml\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import re\n",
    "\n",
    "from matplotlib import pyplot as plt, cm\n",
    "import numpy as np\n",
    "from pandas_path import path\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import make_scorer, log_loss\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "import joblib\n",
    "\n",
    "from datetime import datetime\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "RANDOM_SEED = 42  # For reproducibility\n",
    "random.seed(RANDOM_SEED)\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba25ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mzml directory\n",
    "data_directory = os.path.join(os.getcwd(), 'data')\n",
    "mzml_directory = os.path.join(data_directory, 'MZML Data', 'Compound X')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b7e2fc",
   "metadata": {},
   "source": [
    "# Converts .mzml to excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f4dc5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize variables for tracking iterations and total execution time\n",
    "iteration_count = 0\n",
    "total_execution_time = 0\n",
    "\n",
    "# Iterate through all files in the mzml directory\n",
    "for file in os.listdir(mzml_directory):\n",
    "    if file.endswith(\".mzML\"):\n",
    "        # Load the mzML file\n",
    "        mzml_file = os.path.join(mzml_directory, file)\n",
    "\n",
    "        # Start the timer\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Create a list to store dictionaries representing rows in the DataFrame\n",
    "        rows = []\n",
    "\n",
    "        with mzml.read(mzml_file) as reader:\n",
    "            scan_number = 1\n",
    "            for spectrum in reader:\n",
    "                mz_values = spectrum['m/z array']\n",
    "                intensity_values = spectrum['intensity array']\n",
    "                scan_time = spectrum['scanList']['scan'][0]['scan start time']\n",
    "                tic = spectrum['total ion current']\n",
    "\n",
    "                # Iterate over the mz_values and intensity_values lists\n",
    "                for mz, intensity in zip(mz_values, intensity_values):\n",
    "                    # Create a dictionary representing a row in the DataFrame\n",
    "                    row = {'scan number': scan_number, 'scan time': scan_time, 'total ion current': tic, \n",
    "                           'm/z': mz, 'intensity': intensity, 'normalised intensity': intensity/max(intensity_values), \n",
    "                          'rounded m/z': round(mz)} \n",
    "                    rows.append(row)\n",
    "\n",
    "                scan_number += 1\n",
    "        # Create DataFrame from the list of dictionaries\n",
    "        df_extracted = pd.DataFrame(rows)\n",
    "\n",
    "        # Save the extracted DataFrame as an Excel file with a unique name\n",
    "        cleaned_data_file = os.path.splitext(file)[0] + \"_cleaned.xlsx\"\n",
    "        cleaned_data_path = os.path.join(mzml_directory, cleaned_data_file)\n",
    "        df_extracted.to_excel(cleaned_data_path, index=True)\n",
    "\n",
    "        # Calculate the iteration execution time\n",
    "        iteration_time = time.time() - start_time\n",
    "\n",
    "        # Update iteration count and total execution time\n",
    "        iteration_count += 1\n",
    "        total_execution_time += iteration_time\n",
    "\n",
    "        print(f\"Iteration {iteration_count}: Processed {file} in {iteration_time:.2f} seconds and saved the cleaned data in {cleaned_data_file}.\")\n",
    "\n",
    "# Print the final summary\n",
    "print(f\"\\nTotal iterations: {iteration_count}\")\n",
    "print(f\"Total execution time: {total_execution_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871aca9f",
   "metadata": {},
   "source": [
    "# Move excel to all_features folder (takes into account existing samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cb75ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise list for separate train-test split\n",
    "separate_train_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c625307e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the source and destination folders\n",
    "source_folder = os.path.join(data_directory, 'MZML DATA', 'Compound X')\n",
    "destination_folder = os.path.join(data_directory, 'all_features')\n",
    "\n",
    "# Create the \"master list\" folder if it doesn't exist\n",
    "os.makedirs(destination_folder, exist_ok=True)\n",
    "\n",
    "# Get the list of Excel files in the source folder\n",
    "excel_files = [file for file in os.listdir(source_folder) if file.endswith('.xlsx')]\n",
    "\n",
    "# Determine the starting sample number\n",
    "if excel_files:\n",
    "    last_sample_number = 0\n",
    "    for file in excel_files:\n",
    "        try:\n",
    "            sample_number = int(file.split('_')[-1].split('.')[0])\n",
    "            if sample_number > last_sample_number:\n",
    "                last_sample_number = sample_number\n",
    "        except ValueError:\n",
    "            continue\n",
    "    start_sample_number = last_sample_number + 1\n",
    "else:\n",
    "    start_sample_number = 1\n",
    "\n",
    "# List to store the names of created files\n",
    "created_files = []\n",
    "\n",
    "# Iterate over the Excel files and copy them to the destination folder\n",
    "for file in excel_files:\n",
    "    source_path = os.path.join(source_folder, file)\n",
    "    destination_name = ''\n",
    "    while True:\n",
    "        destination_name = f'sample_{start_sample_number:06}.xlsx'\n",
    "        destination_path = os.path.join(destination_folder, destination_name)\n",
    "\n",
    "        # Check if the destination file already exists\n",
    "        if not os.path.exists(destination_path):\n",
    "            break\n",
    "\n",
    "        start_sample_number += 1\n",
    "\n",
    "    shutil.copyfile(source_path, destination_path)\n",
    "    created_files.append(destination_name)  # Append the created file name to the list\n",
    "    print(f\"File '{file}' copied to '{destination_name}'\")\n",
    "    start_sample_number += 1\n",
    "\n",
    "# Append into list for separate train-test split \n",
    "for file in created_files:\n",
    "    separate_train_test.append(file)\n",
    "\n",
    "print(\"Created files:\")\n",
    "for file_name in created_files:\n",
    "    print(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b478c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(separate_train_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1f0b8c",
   "metadata": {},
   "source": [
    "# Get compound label for added samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c989d704",
   "metadata": {},
   "outputs": [],
   "source": [
    "compounds = []\n",
    "\n",
    "while True:\n",
    "    compound = input(\"What compound is present in the samples? \")\n",
    "\n",
    "    # Check if the input contains only letters and spaces\n",
    "    if re.match(r'^[a-zA-Z\\s\\d!@#$%^&*()]+$', compound):\n",
    "        compounds.append(compound)\n",
    "    else:\n",
    "        print(\"Invalid input. Please enter a compound name containing only letters and spaces.\")\n",
    "\n",
    "    # Ask if there are any other compounds present\n",
    "    response = input(\"Are there any other compounds present? (Y/N) \")\n",
    "    if response.upper() == 'N':\n",
    "        break  # Exit the loop if no more compounds are present\n",
    "\n",
    "# Print the list of compounds\n",
    "print(\"Compounds present in the samples:\")\n",
    "for compound in compounds:\n",
    "    print(compound)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4990e9d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_path = os.path.join(data_directory, 'all_labels.csv')\n",
    "\n",
    "# Remove \".xlsx\" from the elements in created_files\n",
    "created_files = [file.replace('.xlsx', '') for file in created_files]\n",
    "\n",
    "# Check if all_labels.csv already exists in the current directory\n",
    "if 'all_labels.csv' not in os.listdir(data_directory):\n",
    "    # Create all_labels DataFrame with \"sample id\"\n",
    "    all_labels = pd.DataFrame({'sample id': created_files})\n",
    "\n",
    "else:\n",
    "    # Load all_labels DataFrame from the existing CSV file\n",
    "    all_labels = pd.read_csv(file_path)    \n",
    "\n",
    "    # Create a DataFrame from the created_files list\n",
    "    df_new = pd.DataFrame({'sample id': created_files})\n",
    "\n",
    "    # Concatenate all_labels and df_new to add the new rows\n",
    "    all_labels = pd.concat([all_labels, df_new], ignore_index=True)\n",
    "\n",
    "# Check if compounds are already present in all_labels, starting from the second column\n",
    "for compound in compounds:\n",
    "    if compound not in all_labels.iloc[0, 1:]:\n",
    "        # If the compound is not present, create a new column with the compound name\n",
    "        all_labels[compound] = ''\n",
    "        print(f\"Compound '{compound}' added into the labels.\")\n",
    "    else:\n",
    "        print(f\"Compound '{compound}' already exists in the labels.\")\n",
    "        \n",
    "# Check if compounds are already present in all_labels, starting from the second column\n",
    "for compound in compounds:\n",
    "    if compound not in all_labels.iloc[0, 1:]:\n",
    "        # If the compound is not present, create a new column with the compound name\n",
    "        all_labels[compound] = ''\n",
    "        print(f\"Compound '{compound}' added into the labels.\")\n",
    "    else:\n",
    "        print(f\"Compound '{compound}' already exists in the labels.\")\n",
    "\n",
    "# Iterate over the rows and columns to perform one-hot encoding\n",
    "for index, row in all_labels.iterrows():\n",
    "    if row['sample id'] in created_files:\n",
    "        # Set the compounds in created_files list to 1, others to 0\n",
    "        for compound in all_labels.columns[1:]:\n",
    "            if compound in compounds:\n",
    "                if compound in row:\n",
    "                    all_labels.loc[index, compound] = 1\n",
    "                else:\n",
    "                    all_labels.loc[index, compound] = 0\n",
    "            else:\n",
    "                all_labels.loc[index, compound] = 0\n",
    "    else:\n",
    "        # Set compounds to 0 for rows not in created_files list if not already 1\n",
    "        for compound in compounds:\n",
    "            if compound in all_labels.columns and all_labels.loc[index, compound] != 1:\n",
    "                all_labels.loc[index, compound] = 0\n",
    "\n",
    "# Function to convert values to float64 data type\n",
    "def convert_to_float(value):\n",
    "    if isinstance(value, (int, float)):\n",
    "        return float(value)\n",
    "    return value\n",
    "\n",
    "# Apply the conversion to all cells in the DataFrame excluding headers and indexes\n",
    "all_labels.iloc[:, 1:] = all_labels.iloc[:, 1:].applymap(lambda x: convert_to_float(x))\n",
    "\n",
    "# Save all_labels DataFrame to all_labels.csv\n",
    "all_labels.to_csv(file_path, index=False)\n",
    "all_labels.head()\n",
    "\n",
    "# Create a new DataFrame with compound counts\n",
    "compound_counts = all_labels.iloc[:, 1:].sum().sort_values(ascending=True)\n",
    "\n",
    "# Plot the horizontal bar graph\n",
    "plt.barh(compound_counts.index, compound_counts.values)\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Compounds')\n",
    "plt.title('Compounds represented in dataset')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2627ecef",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7bb139",
   "metadata": {},
   "source": [
    "# Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb18180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the source folder (master list) and destination folders (train_features, test_features)\n",
    "source_folder = os.path.join(data_directory, \"all_features\")\n",
    "train_folder = os.path.join(data_directory, \"train_features\")\n",
    "test_folder = os.path.join(data_directory, \"test_features\")\n",
    "\n",
    "# Create the destination folders if they don't exist\n",
    "os.makedirs(train_folder, exist_ok=True)\n",
    "os.makedirs(test_folder, exist_ok=True)\n",
    "\n",
    "# Shuffle the list of files\n",
    "random.shuffle(separate_train_test)\n",
    "\n",
    "# Determine the split point for train/test\n",
    "split_point = int(len(separate_train_test) * 0.8)  # 80% for training, 20% for testing\n",
    "\n",
    "# Split the files into train and test\n",
    "train_files = separate_train_test[:split_point]\n",
    "test_files = separate_train_test[split_point:]\n",
    "\n",
    "# Move the train files to the train folder\n",
    "for file in train_files:\n",
    "    source_path = os.path.join(source_folder, file)\n",
    "    destination_path = os.path.join(train_folder, file)\n",
    "    shutil.copy(source_path, destination_path)\n",
    "\n",
    "# Move the test files to the test folder\n",
    "for file in test_files:\n",
    "    source_path = os.path.join(source_folder, file)\n",
    "    destination_path = os.path.join(test_folder, file)\n",
    "    shutil.copy(source_path, destination_path)\n",
    "\n",
    "print(\"Train/Test split completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96005bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the all_labels.csv file\n",
    "all_labels = pd.read_csv(file_path , index_col=0)  # Assuming the first column is the sample id\n",
    "\n",
    "# Create a counter to store the occurrences of column headers for train and test\n",
    "train_column_occurrences = {}\n",
    "test_column_occurrences = {}\n",
    "\n",
    "# Function to count the occurrences of column headers\n",
    "def count_column_occurrences(file_name, column_occurrences):\n",
    "    # Check if the file name exists in all_labels\n",
    "    if file_name in all_labels.index:\n",
    "        # Get the columns with value 1 for the corresponding file\n",
    "        columns = all_labels.columns[all_labels.loc[file_name] == 1]\n",
    "\n",
    "        # Iterate over the columns and count the occurrences of column headers\n",
    "        for column in columns:\n",
    "            column_header = all_labels.columns[all_labels.loc[file_name] == 1][0]\n",
    "            if column_header in column_occurrences:\n",
    "                column_occurrences[column_header] += 1\n",
    "            else:\n",
    "                column_occurrences[column_header] = 1\n",
    "\n",
    "# Iterate over the files in the train folder\n",
    "for file in os.listdir(train_folder):\n",
    "    if file.endswith(\".xlsx\"):\n",
    "        # Remove the file extension\n",
    "        file_name = os.path.splitext(file)[0]\n",
    "        count_column_occurrences(file_name, train_column_occurrences)\n",
    "\n",
    "# Iterate over the files in the test folder\n",
    "for file in os.listdir(test_folder):\n",
    "    if file.endswith(\".xlsx\"):\n",
    "        # Remove the file extension\n",
    "        file_name = os.path.splitext(file)[0]\n",
    "        count_column_occurrences(file_name, test_column_occurrences)\n",
    "\n",
    "# Sort the column occurrences dictionaries by count in descending order\n",
    "sorted_train_column_occurrences = dict(sorted(train_column_occurrences.items(), key=lambda item: item[1]))\n",
    "sorted_test_column_occurrences = dict(sorted(test_column_occurrences.items(), key=lambda item: item[1]))\n",
    "\n",
    "# Plot the horizontal bar chart for train\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(list(sorted_train_column_occurrences.keys()), list(sorted_train_column_occurrences.values()))\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Compounds')\n",
    "plt.title('Compounds in Train dataset')\n",
    "\n",
    "# Plot the horizontal bar chart for test\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(list(sorted_test_column_occurrences.keys()), list(sorted_test_column_occurrences.values()))\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Compounds')\n",
    "plt.title('Compounds in Test dataset')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeae72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of files in the train_folder directory\n",
    "files = os.listdir(train_folder)\n",
    "\n",
    "# Extract file names without the extension and store them in file_names list\n",
    "file_names = [os.path.splitext(file)[0] for file in files]\n",
    "\n",
    "# Create the train_labels and test_labels DataFrame\n",
    "train_labels = pd.DataFrame(columns=all_labels.columns)\n",
    "test_labels = pd.DataFrame(columns=all_labels.columns)\n",
    "\n",
    "# Set the index header\n",
    "train_labels.index.name = \"sample id\"\n",
    "test_labels.index.name = \"sample id\"\n",
    "\n",
    "# Copy rows that match the values in file_names into train_labels\n",
    "train_labels = pd.concat([train_labels, all_labels.loc[file_names]], axis=0)\n",
    "\n",
    "# Copy rows that are not in file_names into test_labels\n",
    "test_labels = pd.concat([test_labels, all_labels.loc[~all_labels.index.isin(file_names)]], axis=0)\n",
    "\n",
    "# Define the file paths\n",
    "train_labels_path = os.path.join(data_directory, 'train_labels.csv')\n",
    "test_labels_path = os.path.join(data_directory, 'test_labels.csv')\n",
    "\n",
    "# Reads existing labels in order to get the indexes, this is to find the new samples that were added into the train labels\n",
    "read_train_labels = pd.read_csv(train_labels_path)\n",
    "\n",
    "# Get the sample id column as a list without the header\n",
    "old_sample_list = read_train_labels.iloc[:, 0].tolist()\n",
    "\n",
    "# Add \".xlsx\" to the back of each element in old_sample_list\n",
    "old_sample_list = [sample + \".xlsx\" for sample in old_sample_list]\n",
    "\n",
    "# Overrides the old csv for the labels\n",
    "# Save train_labels in the data_directory folder\n",
    "train_labels.to_csv(train_labels_path)\n",
    "\n",
    "# Save test_labels in the data_directory folder\n",
    "test_labels.to_csv(test_labels_path)\n",
    "\n",
    "# Sort new train files in ascending order\n",
    "new_sample_list = sorted(train_files)\n",
    "\n",
    "# Find the difference between the two lists (new/old) using list comprehension\n",
    "difference_list = [sample for sample in new_sample_list if sample not in old_sample_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abc9ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(difference_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc585c5",
   "metadata": {},
   "source": [
    "# Start of feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c226035",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJ_ROOT = Path.cwd().parent\n",
    "DATA_PATH = PROJ_ROOT / \"MAIN CODE/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2020276d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(DATA_PATH / \"all_labels.csv\", index_col=\"sample id\")\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c5b2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a series of time bins\n",
    "timerange = pd.interval_range(start=0, end=35, freq=0.25)\n",
    "timerange\n",
    "\n",
    "# Make dataframe with rows that are combinations of all temperature bins and all m/z values\n",
    "allcombs = list(itertools.product(timerange, [*range(1, 301)]))\n",
    "\n",
    "allcombs_df = pd.DataFrame(allcombs, columns=[\"time bin\", \"rounded m/z\"])\n",
    "print(allcombs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84133c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_per_timebin(df):\n",
    "\n",
    "    \"\"\"\n",
    "    Transforms dataset to take the preprocessed max abundance for each\n",
    "    time range for each m/z value\n",
    "\n",
    "    Args:\n",
    "        df: dataframe to transform\n",
    "\n",
    "    Returns:\n",
    "        transformed dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # Bin times\n",
    "    df[\"time bin\"] = pd.cut(df[\"scan time\"], bins=timerange)\n",
    "\n",
    "    # Combine with a list of all time bin-m/z value combinations\n",
    "    df = pd.merge(allcombs_df, df, on=[\"time bin\", \"rounded m/z\"], how=\"left\")\n",
    "\n",
    "    # Aggregate to time bin level to find max\n",
    "    df = df.groupby([\"time bin\", \"rounded m/z\"]).max(\"normalised intensity\").reset_index()\n",
    "\n",
    "    # Fill in 0 for intensity values without information\n",
    "    df = df.replace(np.nan, 0)\n",
    "\n",
    "    # Reshape so each row is a single sample\n",
    "    df = df.pivot_table(\n",
    "        columns=[\"rounded m/z\", \"time bin\"], values=[\"normalised intensity\"]\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723037c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dict = {file_name[:-5]: \"train_features\\\\\" + file_name for file_name in difference_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8b24c9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(file_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9212baa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assembling preprocessed and transformed training set\n",
    "\n",
    "train_features_dict = {}\n",
    "print(\"Total number of train files: \", len(file_dict))\n",
    "\n",
    "for i, (sample_id, filepath) in enumerate(tqdm(file_dict.items())):\n",
    "    # Load training sample\n",
    "    temp = pd.read_excel(os.path.join(data_directory, filepath))\n",
    "\n",
    "    # Feature engineering\n",
    "    train_sample_fe = int_per_timebin(temp).reset_index(drop=True)\n",
    "    train_features_dict[sample_id] = train_sample_fe\n",
    "\n",
    "train_features = pd.concat(\n",
    "    train_features_dict, names=[\"sample id\", \"dummy index\"]\n",
    ").reset_index(level=\"dummy index\", drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaae62b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_features_new = train_features.sort_index(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444685ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e48ca4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2c52f5",
   "metadata": {},
   "source": [
    "# Reading of existing features df and combining of the 2 df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff31dc2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read the \"train_features_index.csv\" file\n",
    "index_df = pd.read_csv(os.path.join(data_directory, \"train_features_df\", \"train_features_index.csv\"))\n",
    "\n",
    "# Set the \"sample id\" column as the index for \"index_df\"\n",
    "index_df.set_index(\"sample id\", inplace=True)\n",
    "\n",
    "# Read the \"train_features_columns.csv\" file\n",
    "columns_df = pd.read_csv(os.path.join(data_directory, \"train_features_df\", \"train_features_columns.csv\"))\n",
    "\n",
    "# Create a MultiIndex for the columns using \"columns_df\"\n",
    "columns = pd.MultiIndex.from_frame(columns_df)\n",
    "\n",
    "# Read the \"train_features_data.csv\" file\n",
    "data_df = pd.read_csv(os.path.join(data_directory, \"train_features_df\", \"train_features_data.csv\"), header=None)\n",
    "\n",
    "# Create the DataFrame \"train_features\" using the loaded data, index, and columns\n",
    "train_features_existing = pd.DataFrame(data_df.values, index=index_df.index, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78675126",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_existing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813a18c6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_features_existing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0dcf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let both Dataframes' columns be the same\n",
    "train_features_new.columns = train_features_existing.columns\n",
    "# Concatenate the 2 DataFrames\n",
    "train_features = pd.concat([train_features_existing, train_features_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9795fe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37af1c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73baa858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that all sample IDs in features and labels are identical\n",
    "assert train_features.index.equals(train_labels.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716d7766",
   "metadata": {},
   "source": [
    "# Backup of main feature dataframe as 3 separated .csv files \n",
    "# (data, index and columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29414688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the \"train_features_df\" folder if it does not exist\n",
    "train_features_folder = os.path.join(data_directory, \"train_features_df\")\n",
    "if not os.path.exists(train_features_folder):\n",
    "    os.makedirs(train_features_folder)\n",
    "\n",
    "# Save the index to a separate CSV file in the \"train_features_df\" folder\n",
    "train_features.index.to_frame(index=False).to_csv(\n",
    "    os.path.join(train_features_folder, \"train_features_index.csv\")\n",
    ")\n",
    "\n",
    "# Save the columns to a separate CSV file in the \"train_features_df\" folder\n",
    "columns_df = pd.DataFrame(\n",
    "    {\n",
    "        \"rounded m/z\": train_features.columns.get_level_values(0),\n",
    "        \"time bin\": train_features.columns.get_level_values(1),\n",
    "    }\n",
    ")\n",
    "columns_df.to_csv(\n",
    "    os.path.join(train_features_folder, \"train_features_columns.csv\"), index=False\n",
    ")\n",
    "\n",
    "# Save the data (non-index and non-column cells) to a separate CSV file in the \"train_features_df\" folder\n",
    "data_df = train_features.reset_index(drop=True)\n",
    "data_df.to_csv(\n",
    "    os.path.join(train_features_folder, \"train_features_data.csv\"), header=False, index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8248d189",
   "metadata": {},
   "source": [
    "# Start of logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4122689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stratified k-fold validation\n",
    "skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "# Define log loss\n",
    "log_loss_scorer = make_scorer(log_loss, needs_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002456ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check log loss score for baseline dummy model\n",
    "def logloss_cross_val(clf, X, y):\n",
    "\n",
    "    # Generate a score for each label class\n",
    "    log_loss_cv = {}\n",
    "    for col in y.columns:\n",
    "\n",
    "        y_col = y[col]  # take one label at a time\n",
    "        log_loss_cv[col] = np.mean(\n",
    "            cross_val_score(clf, X.values, y_col, cv=skf, scoring=log_loss_scorer)\n",
    "        )\n",
    "\n",
    "    avg_log_loss = np.mean(list(log_loss_cv.values()))\n",
    "\n",
    "    return log_loss_cv, avg_log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bf19a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = train_labels.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fdf486",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(target_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57723c6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dummy classifier\n",
    "dummy_clf = DummyClassifier(strategy=\"prior\")\n",
    "\n",
    "print(\"Dummy model cross-validation average log-loss:\")\n",
    "dummy_logloss = logloss_cross_val(dummy_clf, train_features, train_labels[target_cols])\n",
    "pprint(dummy_logloss[0])\n",
    "print(\"\\nAggregate log-loss\")\n",
    "dummy_logloss[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c4fa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Lasso model\n",
    "logreg_clf = LogisticRegression(penalty=\"l1\", solver=\"liblinear\", C=2)\n",
    "print(\"Logistic regression model cross-validation average log-loss:\\n\")\n",
    "logreg_logloss = logloss_cross_val(logreg_clf, train_features, train_labels[target_cols])\n",
    "pprint(logreg_logloss[0])\n",
    "print(\"Aggregate log-loss\")\n",
    "logreg_logloss[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8368a2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_train_and_save(X_train, y_train):\n",
    "\n",
    "    # Initialize dict to hold fitted models\n",
    "    logreg_model_dict = {}\n",
    "\n",
    "    # Create the \"models\" folder if it doesn't exist\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "    # Split into binary classifier for each class\n",
    "    for col in y_train.columns:\n",
    "\n",
    "        y_train_col = y_train[col]  # Train on one class at a time\n",
    "\n",
    "        # Output the trained model, bind this to a var, then use as input to prediction function\n",
    "        clf = LogisticRegression(penalty=\"l1\", solver=\"liblinear\", C=2, random_state=42)\n",
    "\n",
    "        # Train the classifier\n",
    "        clf.fit(X_train.values, y_train_col)\n",
    "\n",
    "        # Save the model to a file in the \"models\" folder\n",
    "        model_filename = os.path.join(\"models\", f\"logreg_model_{col}.joblib\")\n",
    "        joblib.dump(clf, model_filename)\n",
    "\n",
    "        # Store the model in the dictionary\n",
    "        logreg_model_dict[col] = clf\n",
    "\n",
    "    return logreg_model_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffae158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the modified function to train and save the models\n",
    "fitted_logreg_dict = logreg_train_and_save(train_features, train_labels[target_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8ffe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the report_format DataFrame\n",
    "report_format = pd.DataFrame(columns=all_labels.columns)\n",
    "\n",
    "# Set the index header\n",
    "report_format.index.name = \"sample id\"\n",
    "\n",
    "# Copy index that are not in file_names into report_format\n",
    "report_format = pd.concat([report_format, pd.DataFrame(index=all_labels.index[~all_labels.index.isin(file_names)])], axis=0)\n",
    "\n",
    "# Define the file paths\n",
    "report_format_path = os.path.join(data_directory, 'report_format.csv')\n",
    "\n",
    "# Save test_labels in the data_directory folder\n",
    "report_format.to_csv(report_format_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569437ee",
   "metadata": {},
   "source": [
    "# Stop here if you do not want to use the newly trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49397557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise master_test_files list\n",
    "master_test_files = []\n",
    "for file in os.listdir(test_folder):\n",
    "    if file.endswith(\".xlsx\"):\n",
    "        master_test_files.append(file)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774fbc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dict with test sample IDs and paths\n",
    "test_dict = {test_name[:-5]: \"test_features\\\\\" + test_name for test_name in master_test_files}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d746342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import submission format\n",
    "submission_template_df = pd.read_csv(\n",
    "    DATA_PATH / \"report_format.csv\", index_col=\"sample id\"\n",
    ")\n",
    "compounds_order = submission_template_df.columns\n",
    "sample_order = submission_template_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6bd979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_sample(sample_id, fitted_model_dict):\n",
    "\n",
    "    # Import sample\n",
    "    temp_sample = pd.read_excel(DATA_PATH / test_dict[sample_id])\n",
    "\n",
    "    # Feature engineering on sample\n",
    "    temp_sample = int_per_timebin(temp_sample)\n",
    "\n",
    "    # Generate predictions for each class\n",
    "    temp_sample_preds_dict = {}\n",
    "\n",
    "    for compound in compounds_order:\n",
    "        clf = fitted_model_dict[compound]\n",
    "        temp_sample_preds_dict[compound] = clf.predict_proba(temp_sample.values)[:, 1][0]\n",
    "\n",
    "    return temp_sample_preds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0855291b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dataframe to store logreg submissions in\n",
    "final_submission_df = pd.DataFrame(\n",
    "    [\n",
    "        predict_for_sample(sample_id, fitted_logreg_dict)\n",
    "        for sample_id in tqdm(sample_order)\n",
    "    ],\n",
    "    index=sample_order,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763ee16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that columns and rows are the same between final submission and submission format\n",
    "assert final_submission_df.index.equals(submission_template_df.index)\n",
    "assert final_submission_df.columns.equals(submission_template_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cc817e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming final_submission_df is the DataFrame you want to save\n",
    "current_datetime = datetime.now().strftime(\"%d%m%Y_%H%M\")  # Get current date and time as a string\n",
    "file_name = f\"report_{current_datetime}.xlsx\"  # Create the file name with current date and time\n",
    "\n",
    "final_submission_df.to_excel(file_name)\n",
    "\n",
    "# Load the workbook\n",
    "wb = load_workbook(file_name)\n",
    "\n",
    "# Get the first sheet (assuming there is only one sheet in the Excel file)\n",
    "sheet = wb.active\n",
    "\n",
    "# Autofit column widths for all columns\n",
    "for column_cells in sheet.columns:\n",
    "    length = max(len(str(cell.value)) for cell in column_cells)\n",
    "    adjusted_width = length # Add a little padding and adjust to Excel's internal width units\n",
    "    sheet.column_dimensions[column_cells[0].column_letter].width = adjusted_width\n",
    "\n",
    "# Save the updated workbook\n",
    "wb.save(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecacbce5",
   "metadata": {},
   "source": [
    "# End of prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a355bb0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
