{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58a474dd",
   "metadata": {},
   "source": [
    "# Code is split into 3 main parts\n",
    "# 1. Preprocessing of data\n",
    "# 2. Feature engineering of data\n",
    "# 3. Modelling using logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cc3d56",
   "metadata": {},
   "source": [
    "# INPUT FILES: mzml format files in their separated compound folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bcb373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyteomics\n",
    "import pandas as pd\n",
    "import time\n",
    "from pyteomics import mzml\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import re\n",
    "\n",
    "from matplotlib import pyplot as plt, cm\n",
    "import numpy as np\n",
    "from pandas_path import path\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import make_scorer, log_loss\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "import joblib \n",
    "\n",
    "from datetime import datetime\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "RANDOM_SEED = 42  # For reproducibility\n",
    "random.seed(RANDOM_SEED)\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203cfa36",
   "metadata": {},
   "source": [
    "# Start of preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa65da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mzml directory\n",
    "data_directory = os.path.join(os.getcwd(), 'data')\n",
    "mzml_directory = os.path.join(data_directory, 'MZML Data', 'Compound X')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f1af00",
   "metadata": {},
   "source": [
    "# Mentioned further down as well - Online literature normalises against the entire sample run\n",
    "Code in question: \n",
    "'normalised intensity': intensity/max(intensity_values)\n",
    "\n",
    "Replace with: \n",
    "max_intensity_global = max(max_intensity_global, max(intensity_values))\n",
    "\n",
    "'normalised intensity': intensity / max_intensity_global,  # Using global maximum intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93566dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables for tracking iterations and total execution time\n",
    "iteration_count = 0\n",
    "total_execution_time = 0\n",
    "\n",
    "# Iterate through all files in the mzml directory\n",
    "for file in os.listdir(mzml_directory):\n",
    "    if file.endswith(\".mzML\"):\n",
    "        # Load the mzML file\n",
    "        mzml_file = os.path.join(mzml_directory, file)\n",
    "\n",
    "        # Start the timer\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Create a list to store dictionaries representing rows in the DataFrame\n",
    "        rows = []\n",
    "\n",
    "        with mzml.read(mzml_file) as reader:\n",
    "            scan_number = 1\n",
    "            for spectrum in reader:\n",
    "                mz_values = spectrum['m/z array']\n",
    "                intensity_values = spectrum['intensity array']\n",
    "                scan_time = spectrum['scanList']['scan'][0]['scan start time']\n",
    "                tic = spectrum['total ion current']\n",
    "\n",
    "                # Iterate over the mz_values and intensity_values lists\n",
    "                for mz, intensity in zip(mz_values, intensity_values):\n",
    "                    # Create a dictionary representing a row in the DataFrame\n",
    "                    row = {'scan number': scan_number, 'scan time': scan_time, 'total ion current': tic, \n",
    "                           'm/z': mz, 'intensity': intensity, 'normalised intensity': intensity/max(intensity_values), \n",
    "                          'rounded m/z': round(mz)} \n",
    "                    rows.append(row)\n",
    "\n",
    "                scan_number += 1\n",
    "        # Create DataFrame from the list of dictionaries\n",
    "        df_extracted = pd.DataFrame(rows)\n",
    "\n",
    "        # Save the extracted DataFrame as an Excel file with a unique name\n",
    "        cleaned_data_file = os.path.splitext(file)[0] + \"_cleaned.xlsx\"\n",
    "        cleaned_data_path = os.path.join(mzml_directory, cleaned_data_file)\n",
    "        df_extracted.to_excel(cleaned_data_path, index=True)\n",
    "\n",
    "        # Calculate the iteration execution time\n",
    "        iteration_time = time.time() - start_time\n",
    "\n",
    "        # Update iteration count and total execution time\n",
    "        iteration_count += 1\n",
    "        total_execution_time += iteration_time\n",
    "\n",
    "        print(f\"Iteration {iteration_count}: Processed {file} in {iteration_time:.2f} seconds and saved the cleaned data in {cleaned_data_file}.\")\n",
    "\n",
    "# Print the final summary\n",
    "print(f\"\\nTotal iterations: {iteration_count}\")\n",
    "print(f\"Total execution time: {total_execution_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e911ea",
   "metadata": {},
   "source": [
    "# Repeat for all samples by changing the file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393149be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the source and destination folders\n",
    "source_folder = os.path.join(data_directory, 'MZML DATA', 'Compound X')\n",
    "destination_folder = os.path.join(data_directory, 'all_features')\n",
    "\n",
    "# Create the \"master list\" folder if it doesn't exist\n",
    "os.makedirs(destination_folder, exist_ok=True)\n",
    "\n",
    "# Get the list of Excel files in the source folder\n",
    "excel_files = [file for file in os.listdir(source_folder) if file.endswith('.xlsx')]\n",
    "\n",
    "# Determine the starting sample number\n",
    "if excel_files:\n",
    "    last_sample_number = 0\n",
    "    for file in excel_files:\n",
    "        try:\n",
    "            sample_number = int(file.split('_')[-1].split('.')[0])\n",
    "            if sample_number > last_sample_number:\n",
    "                last_sample_number = sample_number\n",
    "        except ValueError:\n",
    "            continue\n",
    "    start_sample_number = last_sample_number + 1\n",
    "else:\n",
    "    start_sample_number = 1\n",
    "\n",
    "# List to store the names of created files\n",
    "created_files = []\n",
    "\n",
    "# Iterate over the Excel files and copy them to the destination folder\n",
    "for file in excel_files:\n",
    "    source_path = os.path.join(source_folder, file)\n",
    "    destination_name = ''\n",
    "    while True:\n",
    "        destination_name = f'sample_{start_sample_number:06}.xlsx'\n",
    "        destination_path = os.path.join(destination_folder, destination_name)\n",
    "\n",
    "        # Check if the destination file already exists\n",
    "        if not os.path.exists(destination_path):\n",
    "            break\n",
    "\n",
    "        start_sample_number += 1\n",
    "\n",
    "    shutil.copyfile(source_path, destination_path)\n",
    "    created_files.append(destination_name)  # Append the created file name to the list\n",
    "    print(f\"File '{file}' copied to '{destination_name}'\")\n",
    "    start_sample_number += 1\n",
    "\n",
    "print(\"Created files:\")\n",
    "for file_name in created_files:\n",
    "    print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa957180",
   "metadata": {},
   "outputs": [],
   "source": [
    "compounds = []\n",
    "\n",
    "while True:\n",
    "    compound = input(\"What compound is present in the samples? \")\n",
    "\n",
    "    # Check if the input contains only letters and spaces\n",
    "    if re.match(r'^[a-zA-Z\\s\\d]+$', compound):\n",
    "        compounds.append(compound)\n",
    "    else:\n",
    "        print(\"Invalid input. Please enter a compound name containing only letters and spaces.\")\n",
    "\n",
    "    # Ask if there are any other compounds present\n",
    "    response = input(\"Are there any other compounds present? (Y/N) \")\n",
    "    if response.upper() == 'N':\n",
    "        break  # Exit the loop if no more compounds are present\n",
    "\n",
    "# Print the list of compounds\n",
    "print(\"Compounds present in the samples:\")\n",
    "for compound in compounds:\n",
    "    print(compound)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b65f44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_path = os.path.join(data_directory, 'all_labels.csv')\n",
    "\n",
    "# Remove \".xlsx\" from the elements in created_files\n",
    "created_files = [file.replace('.xlsx', '') for file in created_files]\n",
    "\n",
    "# Check if all_labels.csv already exists in the current directory\n",
    "if 'all_labels.csv' not in os.listdir(data_directory):\n",
    "    # Create all_labels DataFrame with \"sample id\"\n",
    "    all_labels = pd.DataFrame({'sample id': created_files})\n",
    "\n",
    "else:\n",
    "    # Load all_labels DataFrame from the existing CSV file\n",
    "    all_labels = pd.read_csv(file_path)    \n",
    "\n",
    "    # Create a DataFrame from the created_files list\n",
    "    df_new = pd.DataFrame({'sample id': created_files})\n",
    "\n",
    "    # Concatenate all_labels and df_new to add the new rows\n",
    "    all_labels = pd.concat([all_labels, df_new], ignore_index=True)\n",
    "\n",
    "# Save all_labels DataFrame to all_labels.csv\n",
    "all_labels.to_csv(file_path, index=False)\n",
    "all_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0680111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if compounds are already present in all_labels, starting from the second column\n",
    "for compound in compounds:\n",
    "    if compound not in all_labels.iloc[0, 1:]:\n",
    "        # If the compound is not present, create a new column with the compound name\n",
    "        all_labels[compound] = ''\n",
    "        print(f\"Compound '{compound}' added into the labels.\")\n",
    "    else:\n",
    "        print(f\"Compound '{compound}' already exists in the labels.\")\n",
    "        \n",
    "# Save all_labels DataFrame to all_labels.csv\n",
    "all_labels.to_csv(file_path, index=False)\n",
    "all_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16040351",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterate over the rows and columns to perform one-hot encoding\n",
    "for index, row in all_labels.iterrows():\n",
    "    if row['sample id'] in created_files:\n",
    "        # Set the compounds in created_files list to 1, others to 0\n",
    "        for compound in all_labels.columns[1:]:\n",
    "            if compound in compounds:\n",
    "                if compound in row:\n",
    "                    all_labels.loc[index, compound] = 1\n",
    "                else:\n",
    "                    all_labels.loc[index, compound] = 0\n",
    "            else:\n",
    "                all_labels.loc[index, compound] = 0\n",
    "    else:\n",
    "        # Set compounds to 0 for rows not in created_files list if not already 1\n",
    "        for compound in compounds:\n",
    "            if compound in all_labels.columns and all_labels.loc[index, compound] != 1:\n",
    "                all_labels.loc[index, compound] = 0\n",
    "\n",
    "# Function to convert values to float64 data type\n",
    "def convert_to_float(value):\n",
    "    if isinstance(value, (int, float)):\n",
    "        return float(value)\n",
    "    return value\n",
    "\n",
    "# Apply the conversion to all cells in the DataFrame excluding headers and indexes\n",
    "all_labels.iloc[:, 1:] = all_labels.iloc[:, 1:].applymap(lambda x: convert_to_float(x))\n",
    "\n",
    "# Save all_labels DataFrame to all_labels.csv\n",
    "all_labels.to_csv(file_path, index=False)\n",
    "all_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6a47da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a new DataFrame with compound counts\n",
    "compound_counts = all_labels.iloc[:, 1:].sum().sort_values(ascending=True)\n",
    "\n",
    "# Plot the horizontal bar graph\n",
    "plt.barh(compound_counts.index, compound_counts.values)\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Compounds')\n",
    "plt.title('Compounds represented in dataset')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe18e21e",
   "metadata": {},
   "source": [
    "# Repeat for all samples by changing the file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc51d11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the source folder (master list) and destination folders (train_features, test_features)\n",
    "source_folder = os.path.join(data_directory, \"all_features\")\n",
    "train_folder = os.path.join(data_directory, \"train_features\")\n",
    "test_folder = os.path.join(data_directory, \"test_features\")\n",
    "\n",
    "# Create the destination folders if they don't exist\n",
    "os.makedirs(train_folder, exist_ok=True)\n",
    "os.makedirs(test_folder, exist_ok=True)\n",
    "\n",
    "# Get the list of Excel files in the source folder\n",
    "excel_files = [file for file in os.listdir(source_folder) if file.endswith(\".xlsx\")]\n",
    "\n",
    "# Shuffle the list of files\n",
    "random.shuffle(excel_files)\n",
    "\n",
    "# Determine the split point for train/test\n",
    "split_point = int(len(excel_files) * 0.8)  # 80% for training, 20% for testing\n",
    "\n",
    "# Split the files into train and test\n",
    "train_files = excel_files[:split_point]\n",
    "test_files = excel_files[split_point:]\n",
    "\n",
    "# Move the train files to the train folder\n",
    "for file in train_files:\n",
    "    source_path = os.path.join(source_folder, file)\n",
    "    destination_path = os.path.join(train_folder, file)\n",
    "    shutil.copy(source_path, destination_path)\n",
    "\n",
    "# Move the test files to the test folder\n",
    "for file in test_files:\n",
    "    source_path = os.path.join(source_folder, file)\n",
    "    destination_path = os.path.join(test_folder, file)\n",
    "    shutil.copy(source_path, destination_path)\n",
    "\n",
    "print(\"Train/Test split completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25885101",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read the all_labels.csv file\n",
    "all_labels = pd.read_csv(file_path , index_col=0)  # Assuming the first column is the sample id\n",
    "\n",
    "# Create a counter to store the occurrences of column headers for train and test\n",
    "train_column_occurrences = {}\n",
    "test_column_occurrences = {}\n",
    "\n",
    "# Function to count the occurrences of column headers\n",
    "def count_column_occurrences(file_name, column_occurrences):\n",
    "    # Check if the file name exists in all_labels\n",
    "    if file_name in all_labels.index:\n",
    "        # Get the columns with value 1 for the corresponding file\n",
    "        columns = all_labels.columns[all_labels.loc[file_name] == 1]\n",
    "\n",
    "        # Iterate over the columns and count the occurrences of column headers\n",
    "        for column in columns:\n",
    "            column_header = all_labels.columns[all_labels.loc[file_name] == 1][0]\n",
    "            if column_header in column_occurrences:\n",
    "                column_occurrences[column_header] += 1\n",
    "            else:\n",
    "                column_occurrences[column_header] = 1\n",
    "\n",
    "# Iterate over the files in the train folder\n",
    "for file in os.listdir(train_folder):\n",
    "    if file.endswith(\".xlsx\"):\n",
    "        # Remove the file extension\n",
    "        file_name = os.path.splitext(file)[0]\n",
    "        count_column_occurrences(file_name, train_column_occurrences)\n",
    "\n",
    "# Iterate over the files in the test folder\n",
    "for file in os.listdir(test_folder):\n",
    "    if file.endswith(\".xlsx\"):\n",
    "        # Remove the file extension\n",
    "        file_name = os.path.splitext(file)[0]\n",
    "        count_column_occurrences(file_name, test_column_occurrences)\n",
    "\n",
    "# Sort the column occurrences dictionaries by count in descending order\n",
    "sorted_train_column_occurrences = dict(sorted(train_column_occurrences.items(), key=lambda item: item[1]))\n",
    "sorted_test_column_occurrences = dict(sorted(test_column_occurrences.items(), key=lambda item: item[1]))\n",
    "\n",
    "# Plot the horizontal bar chart for train\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(list(sorted_train_column_occurrences.keys()), list(sorted_train_column_occurrences.values()))\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Compounds')\n",
    "plt.title('Compounds in Train dataset')\n",
    "\n",
    "# Plot the horizontal bar chart for test\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(list(sorted_test_column_occurrences.keys()), list(sorted_test_column_occurrences.values()))\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Compounds')\n",
    "plt.title('Compounds in Test dataset')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b51781",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the list of files in the train_folder directory\n",
    "files = os.listdir(train_folder)\n",
    "\n",
    "# Extract file names without the extension and store them in file_names list\n",
    "file_names = [os.path.splitext(file)[0] for file in files]\n",
    "\n",
    "# Create the train_labels and test_labels DataFrame\n",
    "train_labels = pd.DataFrame(columns=all_labels.columns)\n",
    "test_labels = pd.DataFrame(columns=all_labels.columns)\n",
    "\n",
    "# Set the index header\n",
    "train_labels.index.name = \"sample id\"\n",
    "test_labels.index.name = \"sample id\"\n",
    "\n",
    "# Copy rows that match the values in file_names into train_labels\n",
    "train_labels = pd.concat([train_labels, all_labels.loc[file_names]], axis=0)\n",
    "\n",
    "# Copy rows that are not in file_names into test_labels\n",
    "test_labels = pd.concat([test_labels, all_labels.loc[~all_labels.index.isin(file_names)]], axis=0)\n",
    "\n",
    "# Define the file paths\n",
    "train_labels_path = os.path.join(data_directory, 'train_labels.csv')\n",
    "test_labels_path = os.path.join(data_directory, 'test_labels.csv')\n",
    "\n",
    "# Save train_labels in the data_directory folder\n",
    "train_labels.to_csv(train_labels_path)\n",
    "\n",
    "# Save test_labels in the data_directory folder\n",
    "test_labels.to_csv(test_labels_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54874833",
   "metadata": {},
   "source": [
    "# Start of feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716fd861",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJ_ROOT = Path.cwd().parent\n",
    "DATA_PATH = PROJ_ROOT / \"MAIN CODE/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2914f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(DATA_PATH / \"all_labels.csv\", index_col=\"sample id\")\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0094119",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_indices = [1, 2, 3, 4, 5] #any 5 random samples for Exploratory Data Analysis (EDA)\n",
    "sample_ids_ls = metadata.index[sample_indices].values\n",
    "\n",
    "# Import datasets for EDA \n",
    "sample_data_dict = {}\n",
    "\n",
    "for sample_id in tqdm(sample_ids_ls, desc='Processing'):\n",
    "    sample_data_dict[sample_id] = pd.read_excel(DATA_PATH / 'all_features' / (sample_id + \".xlsx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e1503c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram(sample_df, sample_lab):\n",
    "\n",
    "    # For visual clarity, we will round these intensity values to the nearest whole number and average the intensity.\n",
    "    sample_df[\"m/z\"] = sample_df[\"m/z\"].round()\n",
    "    sample_df = (\n",
    "        sample_df.groupby([\"scan time\", \"m/z\"])[\"intensity\"].aggregate(\"mean\").reset_index()\n",
    "    )\n",
    "\n",
    "    for m in sample_df[\"m/z\"].unique():\n",
    "        plt.plot(\n",
    "            sample_df[sample_df[\"m/z\"] == m][\"scan time\"],\n",
    "            sample_df[sample_df[\"m/z\"] == m][\"intensity\"],\n",
    "        )\n",
    "\n",
    "    plt.title(sample_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe4934c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 5, figsize=(15, 3), constrained_layout=True)\n",
    "fig.suptitle(\"Samples\")\n",
    "fig.supxlabel(\"Time\")\n",
    "fig.supylabel(\"Intensity\")\n",
    "\n",
    "for i in range(0, 5):\n",
    "    sample_lab = sample_ids_ls[i]\n",
    "    sample_df = sample_data_dict[sample_lab]\n",
    "\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plot_spectrogram(sample_df, sample_lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5e565d",
   "metadata": {},
   "source": [
    "# Finetuning of factors can be further explored\n",
    "# Smaller time bin -> Higher accuracy up to a certain extent(?), however more time will be needed to train the model. \n",
    "# Upper limit of 400 is set for the m/z as analytes of interest do not extend up to that range.\n",
    "# Hence size of data would include: \n",
    "# (35/0.25) time bins * 300 rounded m/z = 42000 feature rows per sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29107f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a series of time bins\n",
    "timerange = pd.interval_range(start=0, end=35, freq=0.25)\n",
    "timerange\n",
    "\n",
    "# Make dataframe with rows that are combinations of all temperature bins and all m/z values\n",
    "allcombs = list(itertools.product(timerange, [*range(1, 301)]))\n",
    "\n",
    "allcombs_df = pd.DataFrame(allcombs, columns=[\"time bin\", \"rounded m/z\"])\n",
    "print(allcombs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1f52e6",
   "metadata": {},
   "source": [
    "# Online literature normalises the abundance against the max in the entire sample run \n",
    "# This code normalises against the max in the same scan number\n",
    "# Possible change to be implemented here and in the preprocessing step at the start as mentioned previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec44e7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_per_timebin(df):\n",
    "\n",
    "    \"\"\"\n",
    "    Transforms dataset to take the preprocessed max abundance for each\n",
    "    time range for each m/z value\n",
    "\n",
    "    Args:\n",
    "        df: dataframe to transform\n",
    "\n",
    "    Returns:\n",
    "        transformed dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # Bin times\n",
    "    df[\"time bin\"] = pd.cut(df[\"scan time\"], bins=timerange)\n",
    "\n",
    "    # Combine with a list of all time bin-m/z value combinations\n",
    "    df = pd.merge(allcombs_df, df, on=[\"time bin\", \"rounded m/z\"], how=\"left\")\n",
    "\n",
    "    # Aggregate to time bin level to find max\n",
    "    df = df.groupby([\"time bin\", \"rounded m/z\"]).max(\"normalised intensity\").reset_index()\n",
    "\n",
    "    # Fill in 0 for intensity values without information\n",
    "    df = df.replace(np.nan, 0)\n",
    "\n",
    "    # Reshape so each row is a single sample\n",
    "    df = df.pivot_table(\n",
    "        columns=[\"rounded m/z\", \"time bin\"], values=[\"normalised intensity\"]\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10548ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dict = {file_name[:-5]: \"train_features\\\\\" + file_name for file_name in train_files}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee23ef4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assembling preprocessed and transformed training set\n",
    "\n",
    "train_features_dict = {}\n",
    "print(\"Total number of train files: \", len(file_dict))\n",
    "\n",
    "for i, (sample_id, filepath) in enumerate(tqdm(file_dict.items())):\n",
    "    # Load training sample\n",
    "    temp = pd.read_excel(os.path.join(data_directory, filepath))\n",
    "\n",
    "    # Feature engineering\n",
    "    train_sample_fe = int_per_timebin(temp).reset_index(drop=True)\n",
    "    train_features_dict[sample_id] = train_sample_fe\n",
    "\n",
    "train_features = pd.concat(\n",
    "    train_features_dict, names=[\"sample id\", \"dummy index\"]\n",
    ").reset_index(level=\"dummy index\", drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115a137e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_features = train_features.sort_index(ascending=True)\n",
    "train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e583b5b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5697a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that all sample IDs in features and labels are identical\n",
    "assert train_features.index.equals(train_labels.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcb4fe1",
   "metadata": {},
   "source": [
    "# Backup of main feature dataframe as 3 separated .csv files \n",
    "# (data, index and columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de37497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the \"train_features_df\" folder if it does not exist\n",
    "train_features_folder = os.path.join(data_directory, \"train_features_df\")\n",
    "if not os.path.exists(train_features_folder):\n",
    "    os.makedirs(train_features_folder)\n",
    "\n",
    "# Save the index to a separate CSV file in the \"train_features_df\" folder\n",
    "train_features.index.to_frame(index=False).to_csv(\n",
    "    os.path.join(train_features_folder, \"train_features_index.csv\")\n",
    ")\n",
    "\n",
    "# Save the columns to a separate CSV file in the \"train_features_df\" folder\n",
    "columns_df = pd.DataFrame(\n",
    "    {\n",
    "        \"rounded m/z\": train_features.columns.get_level_values(0),\n",
    "        \"time bin\": train_features.columns.get_level_values(1),\n",
    "    }\n",
    ")\n",
    "columns_df.to_csv(\n",
    "    os.path.join(train_features_folder, \"train_features_columns.csv\"), index=False\n",
    ")\n",
    "\n",
    "# Save the data (non-index and non-column cells) to a separate CSV file in the \"train_features_df\" folder\n",
    "data_df = train_features.reset_index(drop=True)\n",
    "data_df.to_csv(\n",
    "    os.path.join(train_features_folder, \"train_features_data.csv\"), header=False, index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ec54dd",
   "metadata": {},
   "source": [
    "# Start of logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9591b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stratified k-fold validation\n",
    "skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "# Define log loss\n",
    "log_loss_scorer = make_scorer(log_loss, needs_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0d0755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check log loss score for baseline dummy model\n",
    "def logloss_cross_val(clf, X, y):\n",
    "\n",
    "    # Generate a score for each label class\n",
    "    log_loss_cv = {}\n",
    "    for col in y.columns:\n",
    "\n",
    "        y_col = y[col]  # take one label at a time\n",
    "        log_loss_cv[col] = np.mean(\n",
    "            cross_val_score(clf, X.values, y_col, cv=skf, scoring=log_loss_scorer)\n",
    "        )\n",
    "\n",
    "    avg_log_loss = np.mean(list(log_loss_cv.values()))\n",
    "\n",
    "    return log_loss_cv, avg_log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0c23c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = train_labels.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1f7799",
   "metadata": {},
   "source": [
    "# Dummy classifer gives a baseline by \"prior\" strategy which is basically just probabiliy guessing through the distribution of labels present\n",
    "# The lower the log-loss, the better the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cf5a9a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Dummy classifier\n",
    "dummy_clf = DummyClassifier(strategy=\"prior\")\n",
    "\n",
    "print(\"Dummy model cross-validation average log-loss:\")\n",
    "dummy_logloss = logloss_cross_val(dummy_clf, train_features, train_labels[target_cols])\n",
    "pprint(dummy_logloss[0])\n",
    "print(\"\\nAggregate log-loss\")\n",
    "dummy_logloss[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2243fd",
   "metadata": {},
   "source": [
    "# Using a pre-trained logistic regression model instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de19bae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define Lasso model\n",
    "logreg_clf = LogisticRegression(penalty=\"l1\", solver=\"liblinear\", C=2)\n",
    "print(\"Logistic regression model cross-validation average log-loss:\\n\")\n",
    "logreg_logloss = logloss_cross_val(logreg_clf, train_features, train_labels[target_cols])\n",
    "pprint(logreg_logloss[0])\n",
    "print(\"Aggregate log-loss\")\n",
    "logreg_logloss[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d1a58d",
   "metadata": {},
   "source": [
    "# Modelling and saving of logistic regression based off the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5707b04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_train_and_save(X_train, y_train):\n",
    "\n",
    "    # Initialize dict to hold fitted models\n",
    "    logreg_model_dict = {}\n",
    "\n",
    "    # Create the \"models\" folder if it doesn't exist\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "    # Split into binary classifier for each class\n",
    "    for col in y_train.columns:\n",
    "\n",
    "        y_train_col = y_train[col]  # Train on one class at a time\n",
    "\n",
    "        # Output the trained model, bind this to a var, then use as input to prediction function\n",
    "        clf = LogisticRegression(penalty=\"l1\", solver=\"liblinear\", C=2, random_state=42)\n",
    "\n",
    "        # Train the classifier\n",
    "        clf.fit(X_train.values, y_train_col)\n",
    "\n",
    "        # Save the model to a file in the \"models\" folder\n",
    "        model_filename = os.path.join(\"models\", f\"logreg_model_{col}.joblib\")\n",
    "        joblib.dump(clf, model_filename)\n",
    "\n",
    "        # Store the model in the dictionary\n",
    "        logreg_model_dict[col] = clf\n",
    "\n",
    "    return logreg_model_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc0a3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the modified function to train and save the models\n",
    "fitted_logreg_dict = logreg_train_and_save(train_features, train_labels[target_cols])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636ea268",
   "metadata": {},
   "source": [
    "# Testing our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330da62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the report_format DataFrame\n",
    "report_format = pd.DataFrame(columns=all_labels.columns)\n",
    "\n",
    "# Set the index header\n",
    "report_format.index.name = \"sample id\"\n",
    "\n",
    "# Copy index that are not in file_names into report_format\n",
    "report_format = pd.concat([report_format, pd.DataFrame(index=all_labels.index[~all_labels.index.isin(file_names)])], axis=0)\n",
    "\n",
    "# Define the file paths\n",
    "report_format_path = os.path.join(data_directory, 'report_format.csv')\n",
    "\n",
    "# Save test_labels in the data_directory folder\n",
    "report_format.to_csv(report_format_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94298002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dict with test sample IDs and paths\n",
    "test_dict = {test_name[:-5]: \"test_features\\\\\" + test_name for test_name in test_files}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536d6f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import submission format\n",
    "submission_template_df = pd.read_csv(\n",
    "    DATA_PATH / \"report_format.csv\", index_col=\"sample id\"\n",
    ")\n",
    "compounds_order = submission_template_df.columns\n",
    "sample_order = submission_template_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd33d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_sample(sample_id, fitted_model_dict):\n",
    "\n",
    "    # Import sample\n",
    "    temp_sample = pd.read_excel(DATA_PATH / test_dict[sample_id])\n",
    "\n",
    "    # Feature engineering on sample\n",
    "    temp_sample = int_per_timebin(temp_sample)\n",
    "\n",
    "    # Generate predictions for each class\n",
    "    temp_sample_preds_dict = {}\n",
    "\n",
    "    for compound in compounds_order:\n",
    "        clf = fitted_model_dict[compound]\n",
    "        temp_sample_preds_dict[compound] = clf.predict_proba(temp_sample.values)[:, 1][0]\n",
    "\n",
    "    return temp_sample_preds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8666ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe to store logreg submissions in\n",
    "final_submission_df = pd.DataFrame(\n",
    "    [\n",
    "        predict_for_sample(sample_id, fitted_logreg_dict)\n",
    "        for sample_id in tqdm(sample_order)\n",
    "    ],\n",
    "    index=sample_order,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32e622c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that columns and rows are the same between final submission and submission format\n",
    "assert final_submission_df.index.equals(submission_template_df.index)\n",
    "assert final_submission_df.columns.equals(submission_template_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b38d447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming final_submission_df is the DataFrame you want to save\n",
    "current_datetime = datetime.now().strftime(\"%d%m%Y_%H%M\")  # Get current date and time as a string\n",
    "file_name = f\"report_{current_datetime}.csv\"  # Create the file name with current date and time\n",
    "\n",
    "final_submission_df.to_csv(file_name, index=False)\n",
    "\n",
    "# Load the workbook\n",
    "wb = load_workbook(file_name)\n",
    "\n",
    "# Get the first sheet (assuming there is only one sheet in the Excel file)\n",
    "sheet = wb.active\n",
    "\n",
    "# Autofit column widths for all columns\n",
    "for column_cells in sheet.columns:\n",
    "    length = max(len(str(cell.value)) for cell in column_cells)\n",
    "    adjusted_width = length # Add a little padding and adjust to Excel's internal width units\n",
    "    sheet.column_dimensions[column_cells[0].column_letter].width = adjusted_width\n",
    "\n",
    "# Save the updated workbook\n",
    "wb.save(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00f13bc",
   "metadata": {},
   "source": [
    "# End of Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793a994a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
